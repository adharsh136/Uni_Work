{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9734918f-b25c-4c8f-ac2b-a3314094c222",
   "metadata": {},
   "source": [
    "## <center>CITS5508 Lab sheet 1: Implementing a simple learning model</center>\n",
    "\n",
    "**File name: Simple Learning Model.ipynb**<br>\n",
    "**Date:** February 2024<br>\n",
    "\n",
    "This is an example notebook to support you in implementing the simple learning model discussed in the lectures. \n",
    "\n",
    "We have provided some ideas and functions to help you. Use the slide content and your thinking to create the several necessary steps. Your tasks are: \n",
    "\n",
    "- Generate a linearly separable dataset (you can use the function *make_classification*)\n",
    "- Plot the dataset using the provided codes below.\n",
    "- Use the dataset to apply the simple model on it.\n",
    "- Show that it converges for this linearly separable dataset by plotting the decision boundary for the final values of the weights and the bias. \n",
    "- (Optional) Include in the previous plot the decision boundaries for some intermediate steps of the algorithm (that is, non-final values of the weights and bias) to show the algorithm is converging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f526a-17ad-4dc2-b81a-089ba8af22be",
   "metadata": {},
   "source": [
    "### Functions and Classes\n",
    "\n",
    "We usually let the functions and classes defined in the upper part of the code to facilitate code reading and execution.\n",
    "\n",
    "Defining the *Simple_Model* class is one way to implement the model, but several ways exist. You can choose to follow this code or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd963637-45c5-4096-9acc-988184af01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are defining the class of our model\n",
    "class Simple_Model:\n",
    "    def __init__(self, num_features, num_iterations=1000):\n",
    "        self.num_iterations = num_iterations\n",
    "        #...  # init the weights, don't forget the weight for bias\n",
    "ho\n",
    "    def apply_threshold(self, x):\n",
    "        # apply the decision rule here\n",
    "        # ...\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        # the predict function add the bias term, do the weighted sum and call the decision rule (apply_threshold)\n",
    "        #...\n",
    "        #...\n",
    "        #...\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # the train function will apply the weight update rule for the misclassified examples for num_iterations times\n",
    "        for _ in range(self.num_iterations):\n",
    "            #choose one sample (hint: look at np.random.choice() and range() functions\n",
    "            # ... \n",
    "            # ... \n",
    "\n",
    "            #if(prediction!=y[sample_id]): adapt your code here, you may not be using sample_id\n",
    "                # ... we need X with bias again  \n",
    "                # ... the update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250464a-0c1e-4c92-99c8-be0e3b84263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function plot_decision_boundary takes weights, bias, and X and y ranges as input. \n",
    "#It calculates points along the decision boundary using the formula for a line (w1x1 + w2x2 + b = 0) and then\n",
    "#plots the decision boundary using Matplotlib. You can call this function with different weight and bias values to \n",
    "#visualize different decision boundaries.\n",
    "def plot_decision_boundary(weights, bias, X, y):\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # plotting the examples (hint: check what you did for the 2D plot \n",
    "    #plt.scatter ...\n",
    "    #plt.scatter ...\n",
    "\n",
    "    # Generate points along the decision boundary\n",
    "    x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), len(X))\n",
    "\n",
    "    # find the formula for the line w1x1 + w2x2 + b = 0\n",
    "    # x2 = ...\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.plot(x1, x2, label='Decision Boundary')\n",
    "    \n",
    "    plt.title('Linearly Separable Dataset')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad47afa-c9f6-4c19-a944-131696cbfc39",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3348da8-0d2c-4a16-a6a4-fe63049032aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# to display plots in Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#We will do some 3D plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743fad5-2df7-4557-a0fc-87e691b0e21d",
   "metadata": {},
   "source": [
    "### Creating a synthetic data set with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a7e19-2ec7-40ec-b7ed-cb11d93a56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a linearly separable dataset with two classes and two features\n",
    "n_samples = 100\n",
    "X, y = make_classification(n_samples=n_samples, n_features=2, n_classes=2, n_clusters_per_class=1, n_redundant=0, n_informative=1, random_state=42)\n",
    "\n",
    "#Using label classes as -1 and 1 to work with our current algorithm\n",
    "y[y==0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc3cbc-5f41-4ac6-8f8e-f39c994c57fc",
   "metadata": {},
   "source": [
    "### Plotting the data set in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd8c28-ce24-4916-bbe2-bd9b713a8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "#you can plot the examples and have different colors for the different classes using plt.scatter() like the examples in the lecture\n",
    "#...\n",
    "#...\n",
    "plt.title('Linearly Separable Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fddb5-21f5-43ff-a5b9-32e34396a75c",
   "metadata": {},
   "source": [
    "The plot above shows our problem in a two-dimensional space (our feature space is 2D), but our actual problem exists in three dimensions. We have two features describing each example and a third feature that represents the label of the instances. Let's visualize it in 3D. Rotate the picture to see better that all examples of the blue class are positioned at one and all examples of the red class are positioned at zero.\n",
    "\n",
    "Note: to be able to rotate the 3D visualisation, you will need to install the package *ipympl*. You can it by typing inside your environment: \n",
    "\n",
    "*conda install -c conda-forge ipyml*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc031101-98c4-49d5-8d68-83bb0d4b61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset in 3D\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#...\n",
    "#...\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Label')\n",
    "plt.title('Linearly Separable Dataset')\n",
    "plt.legend()\n",
    "\n",
    "# Enable interactive mode for rotation\n",
    "plt.ion()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c817d-5aa7-4bc1-a764-ceee7c7e5c80",
   "metadata": {},
   "source": [
    "### Initialising our model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdddf7-50d9-4699-9906-6115126edd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Simple_Model(num_features=2)\n",
    "\n",
    "#call the train function\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137ec84-e733-470d-9118-c8ad0406f9e9",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaafea6-f662-467e-a8c3-515de033aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_model.weights will have the weights in norm vector. This is of the form [w0,w1,w2], w0 is bias parameter.\n",
    "weights = # ...\n",
    "bias = # ... \n",
    "\n",
    "plot_decision_boundary(weights, bias, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ca8e2-3b2e-4839-a0dd-e44a1c364697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
